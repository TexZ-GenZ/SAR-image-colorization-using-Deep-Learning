{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c192d-6a4e-4228-87ef-4085cbfc40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import random\n",
    "\n",
    "base_path = Path.home() / \"sar_colorization\"\n",
    "data_path = base_path / \"data/v_2\"\n",
    "models_path = base_path / \"wgan_model2\"\n",
    "\n",
    "\n",
    "# Ensure model directory exists\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# TensorBoard setup\n",
    "log_dir = os.path.join(base_path, \"logs\")\n",
    "train_summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Land types\n",
    "land_types = ['agri', 'barrenland', 'grassland', 'urban']\n",
    "\n",
    "# Load PNG images from the directory and prepare the dataset\n",
    "def load_image_paths(dataset_path, land_types):\n",
    "    sar_image_paths = []\n",
    "    opt_image_paths = []\n",
    "\n",
    "    for land_type in land_types:\n",
    "        s1_path = dataset_path / land_type / 's1'\n",
    "        s2_path = dataset_path / land_type / 's2'\n",
    "        \n",
    "        sar_images = sorted(glob.glob(str(s1_path / '*.png')))\n",
    "        opt_images = sorted(glob.glob(str(s2_path / '*.png')))\n",
    "        \n",
    "        assert len(sar_images) == len(opt_images), f\"Mismatch between SAR and optical images in {land_type}.\"\n",
    "        \n",
    "        sar_image_paths.extend(sar_images)\n",
    "        opt_image_paths.extend(opt_images)\n",
    "    \n",
    "    return sar_image_paths, opt_image_paths\n",
    "\n",
    "# Function to read and preprocess images\n",
    "def load_and_preprocess_image(sar_image_path, opt_image_path):\n",
    "    # Load SAR and optical images\n",
    "    sar_image = tf.io.read_file(sar_image_path)\n",
    "    sar_image = tf.io.decode_png(sar_image, channels=1)\n",
    "    sar_image = tf.image.resize(sar_image, [256, 256])\n",
    "    sar_image = tf.cast(sar_image, tf.float32) / 255.0\n",
    "\n",
    "    opt_image = tf.io.read_file(opt_image_path)\n",
    "    opt_image = tf.io.decode_png(opt_image, channels=3)\n",
    "    opt_image = tf.image.resize(opt_image, [256, 256])\n",
    "    opt_image = tf.cast(opt_image, tf.float32) / 255.0\n",
    "\n",
    "    return sar_image, opt_image\n",
    "\n",
    "# Prepare TensorFlow dataset\n",
    "def create_dataset(sar_image_paths, opt_image_paths, batch_size=1):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sar_image_paths, opt_image_paths))\n",
    "    dataset = dataset.shuffle(len(sar_image_paths))  # Shuffle dataset\n",
    "    dataset = dataset.map(lambda sar, opt: load_and_preprocess_image(sar, opt))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Split the dataset into train and test\n",
    "def split_dataset(sar_image_paths, opt_image_paths, split_ratio=0.8):\n",
    "    dataset_size = len(sar_image_paths)\n",
    "    train_size = int(dataset_size * split_ratio)\n",
    "\n",
    "    sar_image_paths_train = sar_image_paths[:train_size]\n",
    "    opt_image_paths_train = opt_image_paths[:train_size]\n",
    "    \n",
    "    sar_image_paths_test = sar_image_paths[train_size:]\n",
    "    opt_image_paths_test = opt_image_paths[train_size:]\n",
    "    \n",
    "    return sar_image_paths_train, opt_image_paths_train, sar_image_paths_test, opt_image_paths_test\n",
    "\n",
    "# Define the generator (Pix2Pix U-Net with depthwise convolution)\n",
    "def build_generator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    def downsample(filters, size, apply_batchnorm=True):\n",
    "        \"\"\"Downsampling layer with depthwise separable convolution.\"\"\"\n",
    "        result = tf.keras.Sequential()\n",
    "        result.add(layers.SeparableConv2D(filters, size, strides=2, padding='same',\n",
    "                                         depthwise_initializer=initializer,\n",
    "                                         pointwise_initializer=initializer,\n",
    "                                         use_bias=False))\n",
    "        if apply_batchnorm:\n",
    "            result.add(layers.BatchNormalization())\n",
    "        result.add(layers.LeakyReLU())\n",
    "        return result\n",
    "\n",
    "    def upsample(filters, size, apply_dropout=False):\n",
    "        \"\"\"Upsampling layer with depthwise separable convolution.\"\"\"\n",
    "        result = tf.keras.Sequential()\n",
    "        result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         use_bias=False))\n",
    "        result.add(layers.BatchNormalization())\n",
    "        if apply_dropout:\n",
    "            result.add(layers.Dropout(0.5))\n",
    "        result.add(layers.ReLU())\n",
    "        return result\n",
    "\n",
    "    inputs = layers.Input(shape=[256, 256, 1])\n",
    "\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),\n",
    "        downsample(128, 4),\n",
    "        downsample(256, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4),\n",
    "        upsample(256, 4),\n",
    "        upsample(128, 4),\n",
    "        upsample(64, 4),\n",
    "    ]\n",
    "\n",
    "    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same',\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  activation='tanh')\n",
    "\n",
    "    x = inputs\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Define the multi-scale PatchGAN discriminator (critic for WGAN-GP)\n",
    "def build_multiscale_patchgan_critic():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    inp = layers.Input(shape=[256, 256, 1], name='input_image')\n",
    "    tar = layers.Input(shape=[256, 256, 3], name='target_image')\n",
    "    x = layers.concatenate([inp, tar])\n",
    "\n",
    "    def critic_block(x, filters, size, strides=2, apply_batchnorm=True):\n",
    "        x = layers.Conv2D(filters, size, strides=strides, padding='same', kernel_initializer=initializer, use_bias=False)(x)\n",
    "        if apply_batchnorm:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        return x\n",
    "\n",
    "    down1 = critic_block(x, 64, 4, apply_batchnorm=False)\n",
    "    down2 = critic_block(down1, 128, 4)\n",
    "    down3 = critic_block(down2, 256, 4)\n",
    "    down4 = critic_block(down3, 512, 4, strides=1)\n",
    "\n",
    "    patch_out = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(down4)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=[patch_out])\n",
    "\n",
    "# Gradient penalty for WGAN-GP (Fixed to pass SAR and Optical images separately to critic)\n",
    "def gradient_penalty(critic, real_images, fake_images, sar_images):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "\n",
    "    # Interpolate between real and fake optical images\n",
    "    interpolated_optical = alpha * real_images + (1 - alpha) * fake_images\n",
    "\n",
    "    # Compute the gradient penalty by passing SAR and interpolated optical separately\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated_optical)\n",
    "        pred = critic([sar_images, interpolated_optical], training=True)  # Pass SAR and interpolated Optical separately\n",
    "    grads = gp_tape.gradient(pred, [interpolated_optical])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "    return gp\n",
    "\n",
    "# Perceptual Loss using VGG19\n",
    "def build_vgg19_perceptual_loss():\n",
    "    vgg = VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "\n",
    "    content_layers = ['block3_conv3', 'block4_conv3']\n",
    "    content_model = Model(inputs=vgg.input, outputs=[vgg.get_layer(layer).output for layer in content_layers])\n",
    "    content_model.trainable = False\n",
    "\n",
    "    def perceptual_loss(y_true, y_pred):\n",
    "        y_true_vgg = content_model(y_true)\n",
    "        y_pred_vgg = content_model(y_pred)\n",
    "        return tf.reduce_mean([tf.reduce_mean(tf.abs(a - b)) for a, b in zip(y_true_vgg, y_pred_vgg)])\n",
    "    \n",
    "    return perceptual_loss\n",
    "\n",
    "# Define WGAN-GP losses\n",
    "def generator_loss(gen_output, target, perceptual_loss_fn):\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    perceptual_loss = perceptual_loss_fn(target, gen_output)\n",
    "    total_gen_loss = l1_loss + perceptual_loss  # No GAN loss here as WGAN doesn't use binary crossentropy\n",
    "    return total_gen_loss, l1_loss, perceptual_loss\n",
    "\n",
    "def critic_loss(real_output, fake_output, gp, lambda_gp=10):\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output) + lambda_gp * gp\n",
    "\n",
    "# Training step with WGAN-GP\n",
    "@tf.function\n",
    "def train_step(input_image, target, generator, critic, generator_optimizer, critic_optimizer, perceptual_loss_fn, lambda_gp=10):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as critic_tape:\n",
    "        gen_output = generator(input_image, training=True)  # Generated optical image\n",
    "\n",
    "        # Real and fake outputs from the critic\n",
    "        real_output = critic([input_image, target], training=True)\n",
    "        fake_output = critic([input_image, gen_output], training=True)\n",
    "\n",
    "        # Calculate gradient penalty\n",
    "        gp = gradient_penalty(critic, target, gen_output, input_image)\n",
    "\n",
    "        # Calculate losses\n",
    "        gen_total_loss, l1_loss, perceptual_loss = generator_loss(gen_output, target, perceptual_loss_fn)\n",
    "        crit_loss = critic_loss(real_output, fake_output, gp, lambda_gp)\n",
    "\n",
    "        # Apply gradients\n",
    "        generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "        critic_gradients = critic_tape.gradient(crit_loss, critic.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "\n",
    "    return gen_total_loss, crit_loss, l1_loss, perceptual_loss, gen_output\n",
    "\n",
    "# PSNR and SSIM calculations\n",
    "def calculate_accuracy(target, gen_output):\n",
    "    psnr = tf.reduce_mean(tf.image.psnr(target, gen_output, max_val=1.0))\n",
    "    ssim = tf.reduce_mean(tf.image.ssim(target, gen_output, max_val=1.0))\n",
    "    return psnr, ssim\n",
    "\n",
    "# Training loop with TensorBoard logging\n",
    "def train(dataset, epochs, checkpoint_dir, n_critic=5, log_image_interval=1500):\n",
    "    # Build generator and critic models\n",
    "    generator = build_generator()\n",
    "    critic = build_multiscale_patchgan_critic()\n",
    "\n",
    "    # Optimizers for both models\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    critic_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "    # Perceptual loss function using VGG19\n",
    "    perceptual_loss_fn = build_vgg19_perceptual_loss()\n",
    "\n",
    "    # Create checkpoint manager for saving models\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                     critic_optimizer=critic_optimizer,\n",
    "                                     generator=generator,\n",
    "                                     critic=critic)\n",
    "    checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    # Restore the latest checkpoint if available\n",
    "    if checkpoint_manager.latest_checkpoint:\n",
    "        print(f\"Restoring from checkpoint: {checkpoint_manager.latest_checkpoint}\")\n",
    "        checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "        initial_epoch = int(checkpoint_manager.latest_checkpoint.split('-')[-1])\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "        initial_epoch = 0\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(initial_epoch, epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        progress_bar = tqdm(dataset, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "        total_gen_loss, total_crit_loss, total_l1_loss, total_perceptual_loss = 0, 0, 0, 0\n",
    "        total_psnr, total_ssim, batch_count = 0, 0, 0\n",
    "\n",
    "        for n, (input_image, target) in enumerate(progress_bar):\n",
    "            # Train the critic n_critic times before updating the generator\n",
    "            for _ in range(n_critic):\n",
    "                _, crit_loss, _, _, _ = train_step(input_image, target, generator, critic, generator_optimizer, critic_optimizer, perceptual_loss_fn, lambda_gp=10)\n",
    "\n",
    "            # Now train the generator\n",
    "            gen_total_loss, _, l1_loss, perceptual_loss, gen_output = train_step(\n",
    "                input_image, target, generator, critic, generator_optimizer, critic_optimizer, perceptual_loss_fn\n",
    "            )\n",
    "\n",
    "            # Calculate PSNR and SSIM\n",
    "            psnr, ssim = calculate_accuracy(target, gen_output)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_gen_loss += gen_total_loss\n",
    "            total_crit_loss += crit_loss\n",
    "            total_l1_loss += l1_loss\n",
    "            total_perceptual_loss += perceptual_loss\n",
    "            total_psnr += psnr\n",
    "            total_ssim += ssim\n",
    "            batch_count += 1\n",
    "\n",
    "            progress_bar.set_postfix(gen_loss=f\"{gen_total_loss:.4f}\", crit_loss=f\"{crit_loss:.4f}\", psnr=f\"{psnr:.4f}\", ssim=f\"{ssim:.4f}\")\n",
    "\n",
    "            # Visualize and log images every `log_image_interval` batches\n",
    "            if (n + 1) % log_image_interval == 0:\n",
    "                visualize_generated_images(input_image, target, gen_output)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    num_images = min(10, input_image.shape[0])\n",
    "                    sar_images = (input_image[:num_images] + 1) / 2.0\n",
    "                    opt_images = (target[:num_images] + 1) / 2.0\n",
    "                    gen_images = (gen_output[:num_images] + 1) / 2.0\n",
    "\n",
    "                    image_rows = []\n",
    "                    for i in range(num_images):\n",
    "                        sar = sar_images[i, :, :, 0]\n",
    "                        opt = opt_images[i]\n",
    "                        gen = gen_images[i]\n",
    "                        row = tf.concat([tf.repeat(sar[:, :, tf.newaxis], 3, axis=2), opt, gen], axis=1)\n",
    "                        image_rows.append(row)\n",
    "                    \n",
    "                    image_grid = tf.concat(image_rows, axis=0)\n",
    "                    tf.summary.image(\"SAR_True_Generated\", image_grid[tf.newaxis, :, :, :], step=step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # Log average metrics for the entire epoch\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('epoch_generator_loss', total_gen_loss / batch_count, step=epoch)\n",
    "            tf.summary.scalar('epoch_critic_loss', total_crit_loss / batch_count, step=epoch)\n",
    "            tf.summary.scalar('epoch_l1_loss', total_l1_loss / batch_count, step=epoch)\n",
    "            tf.summary.scalar('epoch_perceptual_loss', total_perceptual_loss / batch_count, step=epoch)\n",
    "            tf.summary.scalar('epoch_psnr', total_psnr / batch_count, step=epoch)\n",
    "            tf.summary.scalar('epoch_ssim', total_ssim / batch_count, step=epoch)\n",
    "\n",
    "        # Save the model every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_manager.save(checkpoint_number=epoch + 1)\n",
    "            print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "    return generator  # Return the trained generator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualize SAR and Optical Images\n",
    "def visualize_images(dataset):\n",
    "    for sar_image, opt_image in dataset.take(1):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        ax1.imshow(sar_image[0, :, :, 0], cmap='gray')\n",
    "        ax1.set_title('SAR Image')\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2.imshow(opt_image[0, :, :, :])\n",
    "        ax2.set_title('Optical Image')\n",
    "        ax2.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Visualize Generated Images\n",
    "def visualize_generated_images(sar_image, opt_image, gen_output):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    ax1.imshow(sar_image[0, :, :, 0], cmap='gray')\n",
    "    ax1.set_title('SAR Image')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2.imshow(opt_image[0, :, :, :])\n",
    "    ax2.set_title('Optical Image (True)')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    ax3.imshow(gen_output[0, :, :, :])\n",
    "    ax3.set_title('Optical Image (Generated)')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Generate optical images for test data\n",
    "def generate_optical_images(generator, test_dataset):\n",
    "    for sar_image, opt_image in test_dataset:\n",
    "        gen_output = generator(sar_image, training=False)\n",
    "        visualize_generated_images(sar_image, opt_image, gen_output)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    BATCH_SIZE = 4  # Optimal batch size based on memory constraints\n",
    "    EPOCHS = 150    # Increase the epochs for better results\n",
    "\n",
    "    # Load image paths\n",
    "    # sar_image_paths, opt_image_paths = load_image_paths(data_path, land_types)\n",
    "\n",
    "    # Train-test split\n",
    "    # sar_train, opt_train, sar_test, opt_test = split_dataset(sar_image_paths, opt_image_paths, split_ratio=0.8)\n",
    "\n",
    "    # Create TensorFlow datasets\n",
    "    # train_dataset = create_dataset(sar_train, opt_train, BATCH_SIZE)\n",
    "    # test_dataset = create_dataset(sar_test, opt_test, BATCH_SIZE)\n",
    "\n",
    "    # Visualize the first pair of SAR and Optical images\n",
    "    # visualize_images(train_dataset)\n",
    "\n",
    "    # Train the model\n",
    "    # trained_generator = train(train_dataset, EPOCHS, models_path, n_critic=5)\n",
    "\n",
    "    # Generate optical images for all the test data\n",
    "    # generate_optical_images(trained_generator, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be956312-93ba-4682-8ef7-22c7c22c8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming new SAR images are located in this directory\n",
    "new_images_dir = Path.home() / \"sar_colorization\" / \"new_images\"\n",
    "\n",
    "# Path to the saved model checkpoint directory\n",
    "checkpoint_dir = Path.home() / \"sar_colorization\" / \"wgan_model2\"\n",
    "\n",
    "# Load and preprocess the new SAR images (resizing to 256x256)\n",
    "def load_new_sar_images(image_paths):\n",
    "    sar_images = []\n",
    "    for img_path in image_paths:\n",
    "        sar_image = tf.io.read_file(img_path)\n",
    "        sar_image = tf.io.decode_png(sar_image, channels=1)  # Keep it grayscale\n",
    "        sar_image = tf.image.resize(sar_image, [256, 256])   # Resize to 256x256\n",
    "        sar_image = tf.cast(sar_image, tf.float32) / 255.0    # Normalize to [0, 1]\n",
    "        sar_images.append(sar_image)\n",
    "    \n",
    "    # Convert to a batch for inference\n",
    "    return tf.stack(sar_images, axis=0)\n",
    "\n",
    "# Visualize the generated optical images\n",
    "def visualize_generated_images(sar_images, gen_images):\n",
    "    num_images = sar_images.shape[0]\n",
    "    for i in range(num_images):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        ax1.imshow(sar_images[i, :, :, 0], cmap='gray')\n",
    "        ax1.set_title('SAR Image')\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2.imshow(gen_images[i])\n",
    "        ax2.set_title('Generated Optical Image')\n",
    "        ax2.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Load the trained model from checkpoint\n",
    "def load_trained_model(checkpoint_dir):\n",
    "    generator = build_generator()  # Rebuild the generator architecture\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "        print(f\"Restored model from {latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found, ensure the path is correct.\")\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# Main testing function\n",
    "def test_on_new_images(image_paths, checkpoint_dir):\n",
    "    # Load the SAR images\n",
    "    sar_images = load_new_sar_images(image_paths)\n",
    "    \n",
    "    # Load the trained generator model\n",
    "    generator = load_trained_model(checkpoint_dir)\n",
    "    \n",
    "    # Generate optical images\n",
    "    gen_images = generator(sar_images, training=False)\n",
    "    \n",
    "    # Post-process generated images for visualization\n",
    "    gen_images = (gen_images + 1) / 2.0  # Bring the values back to [0, 1] from [-1, 1]\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_generated_images(sar_images, gen_images)\n",
    "\n",
    "# Dynamically get all the SAR image paths (matching the naming convention SAR-Image-*)\n",
    "new_sar_image_paths = glob.glob(str(new_images_dir / \"SAR-Image-*.jpg\"))\n",
    "\n",
    "# Test the model on the new images\n",
    "test_on_new_images(new_sar_image_paths, checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616aac4e-3187-4b0c-b625-86d4f7c508a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to restore the latest checkpoint and print the checkpoint number\n",
    "def load_latest_checkpoint(checkpoint_dir, generator):\n",
    "    # Initialize the checkpoint manager\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    # Restore the latest checkpoint\n",
    "    if checkpoint_manager.latest_checkpoint:\n",
    "        checkpoint.restore(checkpoint_manager.latest_checkpoint).expect_partial()\n",
    "        checkpoint_number = int(checkpoint_manager.latest_checkpoint.split('-')[-1])\n",
    "        print(f\"Restored from checkpoint: {checkpoint_manager.latest_checkpoint}\")\n",
    "        print(f\"Checkpoint number: {checkpoint_number}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found.\")\n",
    "        checkpoint_number = None\n",
    "\n",
    "    return checkpoint_number\n",
    "\n",
    "# Function to visualize SAR, Optical, and Generated images\n",
    "def visualize_sar_opt_gen(sar_image, opt_image, gen_image):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    ax1.imshow(sar_image[0, :, :, 0], cmap='gray')\n",
    "    ax1.set_title('SAR Image')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2.imshow(opt_image[0, :, :, :])\n",
    "    ax2.set_title('Optical Image (True)')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    ax3.imshow(gen_image[0, :, :, :])\n",
    "    ax3.set_title('Optical Image (Generated)')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Generate and visualize images for 15 examples from the test dataset\n",
    "def generate_and_visualize_images(generator, test_dataset, num_images=150):\n",
    "    count = 0\n",
    "    for sar_image, opt_image in test_dataset.take(num_images):\n",
    "        gen_image = generator(sar_image, training=False)\n",
    "        visualize_sar_opt_gen(sar_image, opt_image, gen_image)\n",
    "        count += 1\n",
    "        if count >= num_images:\n",
    "            break\n",
    "\n",
    "# Main block to load checkpoint and visualize images\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths and load the generator\n",
    "    checkpoint_dir = models_path  # Ensure this path matches the one used during training\n",
    "\n",
    "    # Rebuild the generator model (must match the architecture used in training)\n",
    "    generator = build_generator()\n",
    "\n",
    "    # Load the latest checkpoint\n",
    "    checkpoint_number = load_latest_checkpoint(checkpoint_dir, generator)\n",
    "\n",
    "    if checkpoint_number:\n",
    "        # Generate and visualize 15 images from the test dataset\n",
    "        generate_and_visualize_images(generator, train_dataset, num_images=150)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
